{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72632f09",
   "metadata": {},
   "source": [
    "# 模型訓練\n",
    "\n",
    "- 資料載入\n",
    "- Tokenizer\n",
    "- Dataset 建立\n",
    "- BERT 訓練\n",
    "- 模型評估與儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2fed15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功支援 evaluation_strategy\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(output_dir=\"test\", evaluation_strategy=\"epoch\")\n",
    "print(\"✅ 成功支援 evaluation_strategy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b32f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 可用: True\n",
      "GPU 名稱: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA 可用:\", torch.cuda.is_available())\n",
    "print(\"GPU 名稱:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"無\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c72c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 使用裝置: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awww thats a bummer  you shoulda got david car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  awww thats a bummer  you shoulda got david car...      0\n",
       "1  is upset that he cant update his facebook by t...      0\n",
       "2  i dived many times for the ball managed to sav...      0\n",
       "3     my whole body feels itchy and like its on fire      0\n",
       "4  no its not behaving at all im mad why am i her...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 1. 套件與資料匯入\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# 檢查 GPU 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"✅ 使用裝置:\", device)\n",
    "\n",
    "# 讀取已整合的資料集（請確認路徑正確）\n",
    "df = pd.read_csv(\"../data/processed/combined_multilang.csv\")  # 應包含 'text' 與 'label' 欄位\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eec8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 2. 資料切分\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b070cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+---------------+---------------+---------------+--------+\n",
      "| 語言   |   0(負面)數量 |   1(正面)數量 |   0(負面)占比 |   1(正面)占比 | 種類   |\n",
      "+========+===============+===============+===============+===============+========+\n",
      "| ch     |          3842 |          3854 |        0.4992 |        0.5008 | train  |\n",
      "+--------+---------------+---------------+---------------+---------------+--------+\n",
      "| en     |        613870 |        602336 |        0.5047 |        0.4953 | train  |\n",
      "+--------+---------------+---------------+---------------+---------------+--------+\n",
      "| ch     |           957 |           944 |        0.5034 |        0.4966 | val    |\n",
      "+--------+---------------+---------------+---------------+---------------+--------+\n",
      "| en     |        153209 |        150866 |        0.5039 |        0.4961 | val    |\n",
      "+--------+---------------+---------------+---------------+---------------+--------+\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import re\n",
    "# from tabulate import tabulate\n",
    "\n",
    "# # ✅ 檢查是否為中文\n",
    "# def is_chinese(text):\n",
    "#     return bool(re.search(r'[\\u4e00-\\u9fff]', str(text)))\n",
    "\n",
    "# # ✅ 計算語言類別（不修改原始 df）\n",
    "# lang_series = df[\"text\"].apply(lambda x: \"ch\" if is_chinese(x) else \"en\")\n",
    "\n",
    "# # ✅ 切分資料（含語言標籤）\n",
    "# train_texts, val_texts, train_labels, val_labels, train_langs, val_langs = train_test_split(\n",
    "#     df[\"text\"], df[\"label\"], lang_series, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # ✅ 建立 DataFrame 用於統計\n",
    "# train_df = pd.DataFrame({\"text\": train_texts, \"label\": train_labels, \"lang\": train_langs})\n",
    "# val_df = pd.DataFrame({\"text\": val_texts, \"label\": val_labels, \"lang\": val_langs})\n",
    "\n",
    "# # ✅ 自訂統計函數\n",
    "# def compute_custom_stats(df, kind):\n",
    "#     stats = df.groupby([\"lang\", \"label\"]).size().unstack(fill_value=0)\n",
    "#     stats[0] = stats.get(0, 0)\n",
    "#     stats[1] = stats.get(1, 0)\n",
    "#     stats[\"總數\"] = stats[0] + stats[1]\n",
    "#     stats[\"0(負面)占比\"] = stats[0] / stats[\"總數\"]\n",
    "#     stats[\"1(正面)占比\"] = stats[1] / stats[\"總數\"]\n",
    "#     stats[\"種類\"] = kind\n",
    "#     stats = stats.reset_index()\n",
    "#     stats = stats.rename(columns={0: \"0(負面)數量\", 1: \"1(正面)數量\", \"lang\": \"語言\"})\n",
    "#     return stats[[\"語言\", \"0(負面)數量\", \"1(正面)數量\", \"0(負面)占比\", \"1(正面)占比\", \"種類\"]]\n",
    "\n",
    "# # ✅ 合併統計資料\n",
    "# train_stats = compute_custom_stats(train_df, \"train\")\n",
    "# val_stats = compute_custom_stats(val_df, \"val\")\n",
    "# all_stats = pd.concat([train_stats, val_stats], ignore_index=True)\n",
    "\n",
    "# # ✅ 格式化小數點\n",
    "# all_stats[\"0(負面)占比\"] = all_stats[\"0(負面)占比\"].round(4)\n",
    "# all_stats[\"1(正面)占比\"] = all_stats[\"1(正面)占比\"].round(4)\n",
    "\n",
    "# # ✅ 美化表格輸出\n",
    "# print(tabulate(\n",
    "#     all_stats,\n",
    "#     headers=[\"語言\", \"0(負面)數量\", \"1(正面)數量\", \"0(負面)占比\", \"1(正面)占比\", \"種類\"],\n",
    "#     tablefmt=\"grid\",\n",
    "#     showindex=False\n",
    "# ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c648d274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ✅ 3. 載入多語 BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Tokenize 函數\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d512b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 4. 對訓練與驗證資料進行 tokenization\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "val_encodings = tokenize_function(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23c213db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 5. 自訂 Dataset 包裝器\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94700d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 6. 載入模型\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", num_labels=2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80eb847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 7. 定義評估指標\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5afa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 8. 訓練參數設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bert_model\",\n",
    "    do_train=True,\n",
    "    num_train_epochs=1,  # ✅ 先設為 1，看是否會成功跑完儲存\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",               # ✅ 儲存時機\n",
    "    logging_dir=\"outputs/logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,                  # ✅ 最多保留 2 個 checkpoint\n",
    "    fp16=True,                           # ✅ 混合精度訓練（需 GPU）\n",
    "    overwrite_output_dir=True            # ✅ 若資料夾存在，自動覆蓋\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cacf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 9. Trainer 訓練器\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e5c5968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2037/76494 [10:25<6:20:52,  3.26it/s]\n",
      "  3%|▎         | 2130/76494 [00:40<23:35, 52.52it/s]  \n",
      "  0%|          | 42/76494 [00:06<3:09:29,  6.72it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ✅ 10. 開始訓練\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1859\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1857\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2203\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2200\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2202\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2203\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2206\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2207\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2209\u001b[39m ):\n\u001b[32m   2210\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2211\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3138\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs)\u001b[39m\n\u001b[32m   3135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3137\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3138\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.n_gpu > \u001b[32m1\u001b[39m:\n\u001b[32m   3141\u001b[39m     loss = loss.mean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3161\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs)\u001b[39m\n\u001b[32m   3159\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3160\u001b[39m     labels = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3161\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3162\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3163\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 806 (2 times), autocast_decorator.<locals>.decorate_autocast at line 16 (2 times), convert_outputs_to_fp32.<locals>.forward at line 818 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1539\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1531\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1532\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1533\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1534\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1536\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1537\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1539\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1551\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1553\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ✅ 10. 開始訓練\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215016f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 11. 儲存模型與 tokenizer\n",
    "model.save_pretrained(\"models/bert_multilang\")\n",
    "tokenizer.save_pretrained(\"models/bert_multilang\")\n",
    "print(\"✅ 模型與 tokenizer 已儲存至 models/bert_multilang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79008d2e",
   "metadata": {},
   "source": [
    "# 使用checkpoint 繼續訓練\n",
    "前5步驟照執行之前的就好，第6步開始執行以下版本\n",
    "- 重新載入 tokenizer 與模型\n",
    "- 重新定義 training_args 並指定 resume_from_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手動儲存目前model checkpoint\n",
    "trainer.save_model(\"outputs/bert_model_manual_save\")\n",
    "import os\n",
    "save_path = \"outputs/checkpoint-manual\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "trainer.save_model(save_path)  # 儲存模型 (pytorch_model.bin + config.json)\n",
    "tokenizer.save_pretrained(save_path)  # 儲存 tokenizer\n",
    "\n",
    "# 儲存 optimizer / scheduler / trainer state\n",
    "torch.save(trainer.optimizer.state_dict(), os.path.join(save_path, \"optimizer.pt\"))\n",
    "torch.save(trainer.lr_scheduler.state_dict(), os.path.join(save_path, \"scheduler.pt\"))\n",
    "trainer.state.save_to_json(os.path.join(save_path, \"trainer_state.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d70218db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs/bert_model,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=64,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs/bert_model,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.EPOCH,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c6f313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"outputs/checkpoint-manual\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"outputs/checkpoint-manual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3498aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bert_model\",  # 可與原本不同\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"outputs/logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    resume_from_checkpoint=True,  # ✅ 指定要從 checkpoint 繼續\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33abe1b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[32m      3\u001b[39m trainer = Trainer(\n\u001b[32m      4\u001b[39m     model=model,\n\u001b[32m      5\u001b[39m     args=training_args,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     train_dataset=\u001b[43mtrain_dataset\u001b[49m,\n\u001b[32m      7\u001b[39m     eval_dataset=val_dataset,\n\u001b[32m      8\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m trainer.train(resume_from_checkpoint=\u001b[33m\"\u001b[39m\u001b[33moutputs/checkpoint-manual\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=\"outputs/checkpoint-manual\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda4e861",
   "metadata": {},
   "source": [
    "# 先用小資料及測試\n",
    "- 小資料集的載入與分析（1000筆）\n",
    "- tokenizer 與 Dataset 包裝\n",
    "- Trainer 建立與訓練執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65730eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+---------------+---------------+---------------+--------+\n",
      "| 語言   |   0(負面)數量 |   1(正面)數量 |   0(負面)占比 |   1(正面)占比 | 種類   |\n",
      "+========+===============+===============+===============+===============+========+\n",
      "| ch     |             5 |             2 |        0.7143 |        0.2857 | train  |\n",
      "+--------+---------------+---------------+---------------+---------------+--------+\n",
      "| en     |           381 |           412 |        0.4805 |        0.5195 | train  |\n",
      "+--------+---------------+---------------+---------------+---------------+--------+\n",
      "| ch     |             1 |             1 |        0.5    |        0.5    | val    |\n",
      "+--------+---------------+---------------+---------------+---------------+--------+\n",
      "| en     |           101 |            97 |        0.5101 |        0.4899 | val    |\n",
      "+--------+---------------+---------------+---------------+---------------+--------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "\n",
    "# ✅ 判斷是否為中文\n",
    "def is_chinese(text):\n",
    "    return bool(re.search(r'[\\u4e00-\\u9fff]', str(text)))\n",
    "\n",
    "# ✅ 讀取原始資料\n",
    "df = pd.read_csv(\"../data/processed/combined_multilang.csv\")\n",
    "\n",
    "# ✅ 加入語言分類欄位\n",
    "df[\"lang\"] = df[\"text\"].apply(lambda x: \"ch\" if is_chinese(x) else \"en\")\n",
    "\n",
    "# ✅ 抽樣小型資料集（1000 筆）\n",
    "df_sampled = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ✅ 分割訓練集與驗證集\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_sampled[\"text\"], df_sampled[\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "train_langs, val_langs = train_test_split(df_sampled[\"lang\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ 包裝成 DataFrame\n",
    "train_df = pd.DataFrame({\"text\": train_texts, \"label\": train_labels, \"lang\": train_langs})\n",
    "val_df = pd.DataFrame({\"text\": val_texts, \"label\": val_labels, \"lang\": val_langs})\n",
    "\n",
    "# ✅ 統計函數\n",
    "def compute_custom_stats(df, kind):\n",
    "    stats = df.groupby([\"lang\", \"label\"]).size().unstack(fill_value=0)\n",
    "    stats[0] = stats.get(0, 0)\n",
    "    stats[1] = stats.get(1, 0)\n",
    "    stats[\"總數\"] = stats[0] + stats[1]\n",
    "    stats[\"0(負面)占比\"] = stats[0] / stats[\"總數\"]\n",
    "    stats[\"1(正面)占比\"] = stats[1] / stats[\"總數\"]\n",
    "    stats[\"種類\"] = kind\n",
    "    stats = stats.reset_index()\n",
    "    stats = stats.rename(columns={0: \"0(負面)數量\", 1: \"1(正面)數量\", \"lang\": \"語言\"})\n",
    "    return stats[[\"語言\", \"0(負面)數量\", \"1(正面)數量\", \"0(負面)占比\", \"1(正面)占比\", \"種類\"]]\n",
    "\n",
    "# ✅ 印出統計\n",
    "train_stats = compute_custom_stats(train_df, \"train\")\n",
    "val_stats = compute_custom_stats(val_df, \"val\")\n",
    "all_stats = pd.concat([train_stats, val_stats], ignore_index=True)\n",
    "all_stats[\"0(負面)占比\"] = all_stats[\"0(負面)占比\"].round(4)\n",
    "all_stats[\"1(正面)占比\"] = all_stats[\"1(正面)占比\"].round(4)\n",
    "\n",
    "# ✅ 顯示統計\n",
    "print(tabulate(\n",
    "    all_stats,\n",
    "    headers=[\"語言\", \"0(負面)數量\", \"1(正面)數量\", \"0(負面)占比\", \"1(正面)占比\", \"種類\"],\n",
    "    tablefmt=\"grid\",\n",
    "    showindex=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2594d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Desktop\\University\\大三\\下\\自然語言\\final project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# ✅ 載入多語 BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# ✅ Tokenize 函數（和你原來的相同）\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# ✅ 對小資料集的訓練與驗證資料進行 tokenization\n",
    "small_train_encodings = tokenize_function(train_df[\"text\"])\n",
    "small_val_encodings = tokenize_function(val_df[\"text\"])\n",
    "\n",
    "# ✅ Dataset 包裝器\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(int(self.labels[idx]))  # ✅ 確保是 int\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ 包裝小資料集\n",
    "small_train_dataset = SentimentDataset(small_train_encodings, train_df[\"label\"].tolist())\n",
    "small_val_dataset = SentimentDataset(small_val_encodings, val_df[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9d3f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 100/100 [00:15<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6723, 'grad_norm': 5.030803680419922, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 100/100 [00:16<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6522153615951538, 'eval_accuracy': 0.66, 'eval_f1': 0.728, 'eval_precision': 0.5986842105263158, 'eval_recall': 0.9285714285714286, 'eval_runtime': 0.9022, 'eval_samples_per_second': 221.679, 'eval_steps_per_second': 27.71, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:36<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 36.106, 'train_samples_per_second': 22.157, 'train_steps_per_second': 2.77, 'train_loss': 0.6723469543457031, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.6723469543457031, metrics={'train_runtime': 36.106, 'train_samples_per_second': 22.157, 'train_steps_per_second': 2.77, 'total_flos': 52622211072000.0, 'train_loss': 0.6723469543457031, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# ✅ 載入 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# ✅ Tokenize 小資料集\n",
    "small_train_encodings = tokenizer(train_df[\"text\"].tolist(), padding=True, truncation=True, max_length=128)\n",
    "small_val_encodings = tokenizer(val_df[\"text\"].tolist(), padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# ✅ 自訂 Dataset 包裝器\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(int(self.labels[idx]))  # 確保是 int 型態\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# ✅ 包裝小資料集\n",
    "small_train_dataset = SentimentDataset(small_train_encodings, train_df[\"label\"].tolist())\n",
    "small_val_dataset = SentimentDataset(small_val_encodings, val_df[\"label\"].tolist())\n",
    "\n",
    "# ✅ 定義評估指標函數（沿用）\n",
    "def compute_metrics(eval_pred):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# ✅ 載入 BERT 模型並指定要分類 2 類\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs/bert_model_test\",     # 模型儲存路徑\n",
    "    num_train_epochs=1,                         # 訓練 epoch 數量\n",
    "    per_device_train_batch_size=8,              # 每個裝置的訓練批次大小\n",
    "    per_device_eval_batch_size=8,               # 每個裝置的驗證批次大小\n",
    "    evaluation_strategy=\"epoch\",                # 每個 epoch 驗證\n",
    "    save_strategy=\"epoch\",                      # 每個 epoch 儲存模型\n",
    "    logging_dir=\"./outputs/logs\",               # 日誌儲存路徑\n",
    "    logging_strategy=\"epoch\",                   # 每個 epoch 紀錄 log\n",
    "    load_best_model_at_end=True,                # 根據 eval loss 自動載入最佳模型\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ 建立 Trainer 並指定小資料集\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,  # 如果這邊太大，也可以降低 epoch 或 batch size\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ✅ 開始訓練\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882ec278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子：I am very happy with this product!\n",
      "→ 預測結果：正面\n",
      "\n",
      "句子：這東西爛透了。\n",
      "→ 預測結果：正面\n",
      "\n",
      "句子：The service was okay, but not great.\n",
      "→ 預測結果：正面\n",
      "\n",
      "句子：我非常滿意這次的購買經驗。\n",
      "→ 預測結果：正面\n",
      "\n",
      "句子：爛\n",
      "→ 預測結果：正面\n",
      "\n",
      "句子：好\n",
      "→ 預測結果：正面\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 測試\n",
    "test_sentences = [\n",
    "    \"I am very happy with this product!\",\n",
    "    \"這東西爛透了。\",\n",
    "    \"The service was okay, but not great.\",\n",
    "    \"我非常滿意這次的購買經驗。\",\n",
    "    \"爛\",\n",
    "    \"好\"\n",
    "]\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_sentences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# ✅ 搬資料到同一個裝置（模型所在裝置）\n",
    "test_encodings = {k: v.to(model.device) for k, v in test_encodings.items()}\n",
    "\n",
    "# ✅ 預測\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_encodings)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "# ✅ 顯示結果\n",
    "for sentence, pred in zip(test_sentences, predictions):\n",
    "    label = \"正面\" if pred.item() == 1 else \"負面\"\n",
    "    print(f\"句子：{sentence}\\n→ 預測結果：{label}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
